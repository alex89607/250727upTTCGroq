name: TTC Benchmarking Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      policy:
        description: 'Specific policy to test (optional)'
        required: false
        type: string
      benchmark:
        description: 'Specific benchmark to run (optional)'
        required: false
        type: string

env:
  PYTHON_VERSION: '3.11'
  GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      policies: ${{ steps.get-policies.outputs.policies }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Get policy matrix
        id: get-policies
        run: |
          if [ -n "${{ github.event.inputs.policy }}" ]; then
            echo "policies=[\"${{ github.event.inputs.policy }}\"]" >> $GITHUB_OUTPUT
          else
            echo "policies=[\"speculative_decoding\", \"speculative_decoding_4token\", \"dynamic_pruning_top_p\", \"dynamic_pruning_entropy\", \"early_exit_28\", \"early_exit_32\", \"adaptive_kv_static\", \"adaptive_kv_cosine\", \"elastic_batch_greedy\", \"elastic_batch_sorted\"]" >> $GITHUB_OUTPUT
          fi

  benchmark:
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        policy: ${{ fromJson(needs.setup.outputs.policies) }}
        benchmark: ${{ github.event.inputs.benchmark && fromJson(format('["{0}"]', github.event.inputs.benchmark)) || fromJson('["crmarena", "worfbench"]') }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Cache Python dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
            
      - name: Cache model weights (68GB)
        uses: actions/cache@v3
        with:
          path: ~/.cache/huggingface
          key: ${{ runner.os }}-hf-models-${{ hashFiles('**/config.yaml') }}
          restore-keys: |
            ${{ runner.os }}-hf-models-
            
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: Validate environment
        run: |
          python -c "import groq; print('Groq client available')"
          python -c "import pandas as pd; print('Pandas version:', pd.__version__)"
          python -c "import asyncio; print('AsyncIO available')"
          
      - name: Create results directory
        run: mkdir -p results artifacts
        
      - name: Run benchmark
        id: benchmark
        run: |
          echo "Running ${{ matrix.policy }} on ${{ matrix.benchmark }}"
          python runner.py \
            --policy ${{ matrix.policy }} \
            --benchmark ${{ matrix.benchmark }} \
            --output results/results_${{ matrix.policy }}_${{ matrix.benchmark }}.parquet \
            --verbose
            
      - name: Generate summary report
        run: |
          python -c "
          import pandas as pd
          import json
          from pathlib import Path
          
          # Load results
          result_file = 'results/results_${{ matrix.policy }}_${{ matrix.benchmark }}.parquet'
          if Path(result_file).exists():
              df = pd.read_parquet(result_file)
              
              # Calculate summary statistics
              summary = {
                  'policy': '${{ matrix.policy }}',
                  'benchmark': '${{ matrix.benchmark }}',
                  'total_samples': len(df),
                  'avg_first_token_latency': df['first_token_latency'].mean(),
                  'p95_first_token_latency': df['first_token_latency'].quantile(0.95),
                  'avg_throughput': df['throughput'].mean(),
                  'avg_rouge_l': df.get('rouge_l', pd.Series([0])).mean(),
                  'avg_exact_match': df.get('exact_match', pd.Series([0])).mean(),
                  'error_rate': (df['response'].isna().sum() / len(df)) if len(df) > 0 else 1.0
              }
              
              # Save summary
              with open('artifacts/summary_${{ matrix.policy }}_${{ matrix.benchmark }}.json', 'w') as f:
                  json.dump(summary, f, indent=2)
                  
              print(f'Summary saved for {summary[\"policy\"]} on {summary[\"benchmark\"]}')
              print(f'Samples: {summary[\"total_samples\"]}')
              print(f'Avg Latency: {summary[\"avg_first_token_latency\"]:.3f}s')
              print(f'Avg Throughput: {summary[\"avg_throughput\"]:.2f} tokens/s')
          else:
              print('No results file found')
          "
          
      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results-${{ matrix.policy }}-${{ matrix.benchmark }}
          path: |
            results/results_${{ matrix.policy }}_${{ matrix.benchmark }}.parquet
            artifacts/summary_${{ matrix.policy }}_${{ matrix.benchmark }}.json
          retention-days: 30
          
      - name: Upload logs on failure
        if: failure()
        uses: actions/upload-artifact@v3
        with:
          name: logs-${{ matrix.policy }}-${{ matrix.benchmark }}
          path: |
            *.log
            /tmp/*.log
          retention-days: 7

  aggregate:
    needs: benchmark
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas plotly streamlit dbt-core dbt-duckdb
          
      - name: Download all artifacts
        uses: actions/download-artifact@v3
        with:
          path: downloaded-artifacts
          
      - name: Aggregate results
        run: |
          python -c "
          import pandas as pd
          import json
          from pathlib import Path
          import glob
          
          # Combine all parquet files
          parquet_files = glob.glob('downloaded-artifacts/*/results_*.parquet')
          if parquet_files:
              dfs = []
              for file in parquet_files:
                  try:
                      df = pd.read_parquet(file)
                      dfs.append(df)
                  except Exception as e:
                      print(f'Error reading {file}: {e}')
              
              if dfs:
                  combined_df = pd.concat(dfs, ignore_index=True)
                  combined_df.to_parquet('results/combined_results.parquet', index=False)
                  print(f'Combined {len(combined_df)} samples from {len(dfs)} files')
              else:
                  print('No valid parquet files found')
          else:
              print('No parquet files found')
          
          # Combine all summaries
          summary_files = glob.glob('downloaded-artifacts/*/summary_*.json')
          if summary_files:
              summaries = []
              for file in summary_files:
                  try:
                      with open(file, 'r') as f:
                          summary = json.load(f)
                          summaries.append(summary)
                  except Exception as e:
                      print(f'Error reading {file}: {e}')
              
              if summaries:
                  with open('results/combined_summary.json', 'w') as f:
                      json.dump(summaries, f, indent=2)
                  print(f'Combined {len(summaries)} summaries')
              else:
                  print('No valid summary files found')
          else:
              print('No summary files found')
          "
          
      - name: Generate HTML report
        run: |
          python -c "
          import pandas as pd
          import json
          from pathlib import Path
          
          html_content = '''
          <!DOCTYPE html>
          <html>
          <head>
              <title>TTC Benchmark Report</title>
              <style>
                  body { font-family: Arial, sans-serif; margin: 40px; }
                  table { border-collapse: collapse; width: 100%; }
                  th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
                  th { background-color: #f2f2f2; }
                  .metric { background-color: #e8f4fd; }
                  .policy { font-weight: bold; }
              </style>
          </head>
          <body>
              <h1>üöÄ TTC Benchmarking Report</h1>
              <p><strong>Generated:</strong> $(date)</p>
              <p><strong>Workflow:</strong> ${{ github.workflow }} #${{ github.run_number }}</p>
          '''
          
          # Load combined summary if available
          if Path('results/combined_summary.json').exists():
              with open('results/combined_summary.json', 'r') as f:
                  summaries = json.load(f)
              
              html_content += '''
              <h2>Performance Summary</h2>
              <table>
                  <tr>
                      <th>Policy</th>
                      <th>Benchmark</th>
                      <th>Samples</th>
                      <th>Avg Latency (s)</th>
                      <th>P95 Latency (s)</th>
                      <th>Throughput (tok/s)</th>
                      <th>ROUGE-L</th>
                      <th>Exact Match</th>
                      <th>Error Rate</th>
                  </tr>
              '''
              
              for summary in summaries:
                  html_content += f'''
                  <tr>
                      <td class=\"policy\">{summary.get('policy', 'N/A')}</td>
                      <td>{summary.get('benchmark', 'N/A')}</td>
                      <td>{summary.get('total_samples', 0)}</td>
                      <td class=\"metric\">{summary.get('avg_first_token_latency', 0):.3f}</td>
                      <td class=\"metric\">{summary.get('p95_first_token_latency', 0):.3f}</td>
                      <td class=\"metric\">{summary.get('avg_throughput', 0):.2f}</td>
                      <td class=\"metric\">{summary.get('avg_rouge_l', 0):.3f}</td>
                      <td class=\"metric\">{summary.get('avg_exact_match', 0):.3f}</td>
                      <td class=\"metric\">{summary.get('error_rate', 0):.3f}</td>
                  </tr>
                  '''
              
              html_content += '</table>'
          else:
              html_content += '<p>No summary data available</p>'
          
          html_content += '''
              <h2>System Information</h2>
              <ul>
                  <li><strong>Model:</strong> llama-3.3-70b-versatile</li>
                  <li><strong>Infrastructure:</strong> GroqCloud A100x8</li>
                  <li><strong>Concurrency:</strong> 50 requests</li>
                  <li><strong>Rate Limit:</strong> 100 RPS</li>
              </ul>
              
              <h2>Artifacts</h2>
              <ul>
                  <li>Combined Results: <code>results/combined_results.parquet</code></li>
                  <li>Summary Data: <code>results/combined_summary.json</code></li>
                  <li>Individual Results: Available in workflow artifacts</li>
              </ul>
          </body>
          </html>
          '''
          
          with open('results/benchmark_report.html', 'w') as f:
              f.write(html_content)
          
          print('HTML report generated')
          "
          
      - name: Upload combined results
        uses: actions/upload-artifact@v3
        with:
          name: combined-benchmark-results
          path: |
            results/combined_results.parquet
            results/combined_summary.json
            results/benchmark_report.html
          retention-days: 90
          
      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            let comment = '## üöÄ TTC Benchmark Results\n\n';
            
            try {
              const summaryPath = 'results/combined_summary.json';
              if (fs.existsSync(summaryPath)) {
                const summaries = JSON.parse(fs.readFileSync(summaryPath, 'utf8'));
                
                comment += '| Policy | Benchmark | Latency (s) | Throughput (tok/s) | ROUGE-L |\n';
                comment += '|--------|-----------|-------------|-------------------|----------|\n';
                
                summaries.forEach(summary => {
                  comment += `| ${summary.policy} | ${summary.benchmark} | ${summary.avg_first_token_latency?.toFixed(3) || 'N/A'} | ${summary.avg_throughput?.toFixed(2) || 'N/A'} | ${summary.avg_rouge_l?.toFixed(3) || 'N/A'} |\n`;
                });
                
                comment += '\nüìä [View detailed report in artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})\n';
              } else {
                comment += '‚ö†Ô∏è No benchmark results available\n';
              }
            } catch (error) {
              comment += `‚ùå Error generating results: ${error.message}\n`;
            }
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  notify:
    needs: [benchmark, aggregate]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: Notify completion
        run: |
          echo "TTC Benchmarking Pipeline completed"
          echo "Status: ${{ needs.benchmark.result }} / ${{ needs.aggregate.result }}"
          echo "View results: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
